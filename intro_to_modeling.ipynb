{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Modeling\n",
    "\n",
    "Plan ➔ Acquire ➔ Prepare ➔ Explore ➔ **Model** ➔ Deliver\n",
    "\n",
    "Before modeling:\n",
    "\n",
    "0. Split your data\n",
    "1. Data preprocessing\n",
    "\n",
    "The modeling \"loop\"\n",
    "\n",
    "1. Create a model\n",
    "    - algorithm + hyperparams\n",
    "    - training data\n",
    "1. Evaluate the model\n",
    "1. Repeat\n",
    "\n",
    "After a certain amount of time or repititions has passed:\n",
    "\n",
    "1. Compare models\n",
    "1. Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-700f748db601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/classification-exercises/acquire.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     '''\n\u001b[1;32m     14\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mmakes\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpulls\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCodeUp\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# We'll use sklearn's Dummy Classifier as a standin for other classification algorithms\n",
    "# it behaves the same way and we use it the same way that we'll use the \"real\" models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import acquire\n",
    "import prepare\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.prep_titanic_data(acquire.get_titanic_data())\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop(columns='survived'), train.survived\n",
    "X_validate, y_validate = validate.drop(columns='survived'), validate.survived\n",
    "X_test, y_test = test.drop(columns='survived'), test.survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our First Model\n",
    "\n",
    "### Aside: Working with sklearn ML objects\n",
    "\n",
    "1. Create the object\n",
    "1. Fit the object on training data\n",
    "1. Use the object (.score, .predict, .transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the object\n",
    "model = DummyClassifier(strategy='constant', constant=1)\n",
    "# 2. Fit the object\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways we use sklearn classification models:\n",
    "\n",
    "- `.score` gives us accuracy\n",
    "- `.predict` lets us make predictions given a set of indep vars\n",
    "- `.predict_proba` gives us the probability that each observation falls into each label\n",
    "- some specific model types have additional properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracy: %.4f' % model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: view the accuracy on the validate split\n",
    "\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using .predict, the values passed must be the same shape as the values used to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a new column on the train dataframe that contains the models predictions\n",
    "\n",
    "train[\"prediction\"] = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the column you just created and the actual values in the survived column\n",
    "# to generate a classification report\n",
    "\n",
    "pd.DataFrame(classification_report(train.survived, train.prediction, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More models\n",
    "\n",
    "Now we'll make more models, one model is the unique combination of:\n",
    "\n",
    "- algorithm\n",
    "- hyperparameters\n",
    "- training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DummyClassifier(strategy='constant', constant=0)\n",
    "# TODO: fit the model on the training data\n",
    "model1.fit(X_train, y_train)\n",
    "# TODO: see how this model performs on train and validate\n",
    "model1.score(X_train, y_train), model1.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DummyClassifier(strategy='uniform', random_state=0)\n",
    "# TODO: fit the model on the training data\n",
    "model2.fit(X_train, y_train)\n",
    "# TODO: see how this model performs on train and validate\n",
    "model2.score(X_train, y_train), model2.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the pattern above, create 2 more models that vary in either hyperparameters or training data\n",
    "# fit the models and view their performance\n",
    "model3 = DummyClassifier(strategy=\"stratified\", random_state=123)\n",
    "model3.fit(X_train, y_train)\n",
    "model3.score(X_train, y_train), model3.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = DummyClassifier(strategy=\"uniform\", random_state = 13)\n",
    "model4.fit(X_train, y_train)\n",
    "model4.score(X_train, y_train), model4.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are looking for when evaluating model performance is:\n",
    "    \n",
    "    Is the model overfit? Does it perform drastically better on the training data compared to the validate splt\n",
    "    how good or bad is the model? How does the model perform?\n",
    "    - Compared to other models\n",
    "    - Compared to the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare the performance of your models on the validate split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 is our best model with 61% accuracy on validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find the performance of your best model on the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
